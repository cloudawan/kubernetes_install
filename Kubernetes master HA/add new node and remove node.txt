# Adding new Kubernetes Node/Minion (Not Master)

# Linux bridge
sudo apt-get install -y bridge-utils



# Docker (Latest)
#wget -qO- https://get.docker.com/ | sh

# Docker (1.6.1)
#sudo vi /etc/apt/sources.list.d/docker.list
#add "deb http://get.docker.com/ubuntu docker main"
#sudo apt-key adv --keyserver pgp.mit.edu --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9
#sudo apt-get update
#sudo apt-get install -y lxc-docker-1.6.1

# Docker (1.10.3)
#wget -O docker.deb https://apt.dockerproject.org/repo/pool/main/d/docker-engine/docker-engine_1.10.3-0~trusty_amd64.deb
#sudo dpkg -i docker.deb

# Docker (1.10.3)
sudo vi /etc/apt/sources.list.d/docker.list
add "deb https://apt.dockerproject.org/repo ubuntu-trusty main"
sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
sudo apt-get update
sudo apt-get install -y docker-engine=1.10.3-0~trusty



# NTP
sudo apt-get install -y ntp
ntpq -p



# Install python-pip and python httplib2
sudo apt-get update
sudo apt-get install -y python-pip
sudo pip install httplib2



# Use domain name (only lower case is allowed) rather than IP
# If using domain name, all the existing nodes need to update hosts, too.
sudo vi /etc/hosts

Add the following to the new added node
ip1 k8node1 
ip2 k8node2 
ip3 k8node3 
ip4 k8node4

sudo vi /etc/hostname
Modify the hostname



# Copy the files from first node to target nodes
~/kube/reconfDocker.sh

/opt/bin/flanneld
/etc/default/flanneld
/etc/init/flanneld.conf
/etc/init.d/flanneld

/opt/bin/kubelet
/etc/default/kubelet
/etc/init/kubelet.conf
/etc/init.d/kubelet

/opt/bin/kube-proxy
/etc/default/kube-proxy
/etc/init/kube-proxy.conf
/etc/init.d/kube-proxy



# Modify flannel configuration
sudo vi /etc/default/flanneld
Change "--etcd-endpoints=http://127.0.0.1:4001" to the etcd cluster
"--etcd-endpoints=http://127.0.0.1:4001" -> "--etcd-endpoints=http://k8node1:4001,k8node2:4001,k8node3:4001"

sudo service flanneld restart



# Reconfigure Docker
sudo ./reconfDocker.sh i



# Modify kubelet configuration
sudo vi /etc/default/kubelet
Change hostname "--hostname-override=k8node1" to this node "--hostname-override=k8node4"
Change "--api-servers=http://127.0.0.1:8080" to the master cluster "--api-servers=http://k8node1:8080,k8node2:8080,k8node3:8080"
Add "--housekeeping-interval=1s"

KUBELET_OPTS=" --address=0.0.0.0 --port=10250  --hostname-override=k8node1  --api-servers=http://127.0.0.1:8080  --logtostderr=true  --cluster-dns=172.17.3.10  --cluster-domain=cluster.local"

->

KUBELET_OPTS=" --address=0.0.0.0 --port=10250  --hostname-override=k8node4  --api-servers=k8node1:8080,k8node2:8080,k8node3:8080  --logtostderr=true  --cluster-dns=172.17.3.10  --cluster-domain=cluster.local  --housekeeping-interval=1s"


sudo service kubelet restart



# Modify kube-proxy configuration
sudo vi /etc/default/kube-proxy
Change "--master=http://127.0.0.1:8080" to the master cluster "--master=http://k8node1:8080" (Multiple apiserver solution is not supported)

sudo service kube-proxy restart



# Modify flannel upstart since there is no etcd on Node
sudo vi /etc/init/flanneld.conf

Change etcd to network device

	"start on started etcd"

	->

	"start on net-device-up IFACE!=lo"

Change etcd to runlevel
	
	"stop on stopping etcd"

	->

	"stop on runlevel [!2345]"


# Modify docker for dynamically remapping network with flannel for changing of physical IP
# Modify docker configuration
sudo vi /etc/default/docker

DOCKER_OPTS=" -H tcp://127.0.0.1:4243 -H unix:///var/run/docker.sock --bip=172.16.56.1/24 --mtu=1472"

->

DOCKER_OPTS=" -H tcp://127.0.0.1:4243 -H unix:///var/run/docker.sock"

sudo vi /etc/init/docker.conf

add the flannel dependency to start

	start on (local-filesystems and net-device-up IFACE!=lo)

	->

	start on (local-filesystems and net-device-up IFACE!=lo and started flanneld)

add the following above exec "$DOCKER" -d $DOCKER_OPTS in the script

        until [ -f /run/flannel/subnet.env ]
        do
                sleep 1
        done
        FLANNEL_SUBNET=
        FLANNEL_MTU=
        if [ -f /run/flannel/subnet.env ]; then
                . /run/flannel/subnet.env
        fi
        DOCKER_OPTS="$DOCKER_OPTS --bip=$FLANNEL_SUBNET --mtu=$FLANNEL_MTU"
        DOCKER_IP=$(/sbin/ifconfig docker0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
        DOCKER_SUBNET_PREFIX=$(echo $DOCKER_IP | cut -f1,2,3 -d.)
        FLANNEL_SUBNET_PREFIX=$(echo $FLANNEL_SUBNET | cut -f1,2,3 -d.)
        # Reconfigure docker network if it is different
        if [ ! -z "$DOCKER_SUBNET_PREFIX" ] && [ $DOCKER_SUBNET_PREFIX != $FLANNEL_SUBNET_PREFIX ]; then
                sudo ip link set dev docker0 down
                sudo brctl delbr docker0
                sudo iptables -t nat -F
        fi


		
# Health Check
place the files to the following path
/srv/cloudone_node_health/cloudone_node_health.py
/etc/init/cloudone_node_health.conf

# modify the etcd path
sudo vi /srv/cloudone_node_health/cloudone_node_health.py
Change "self.etcd_host_and_port = "http://127.0.0.1:4001"" to "self.etcd_host_and_port = "http://k8nodeb1:4001""

sudo service cloudone_node_health restart



# Glusterfs (On separate machines) Each node only needs Glusterfs client

# Use version 3.7
sudo vi /etc/apt/sources.list
add "deb http://ppa.launchpad.net/gluster/glusterfs-3.7/ubuntu trusty main"
add "deb-src http://ppa.launchpad.net/gluster/glusterfs-3.7/ubuntu trusty main"
# Update
sudo apt-get update

# Install Glusterfs
sudo apt-get install -y --force-yes glusterfs-server

# Allow to use insecure port so the remote docker instance could issue command (No more required since the way to issue command is changed to ssh login)
#sudo vi /etc/glusterfs/glusterd.vol
#add "option rpc-auth-allow-insecure on"
#sudo service glusterfs-server restart

# Check status
#sudo service glusterfs-server status

# Configure the trusted pool
#sudo gluster peer probe <other_node_ip>



# private-registry

1. Docker private registry requires strict HTTPS that verify domain name and client side verify with CA certificate. 
	1. So use the CA and certificate and key in the directory (cacert.pem, private_registry_cert.pem, private_registry_key.pem) where "private-registry" is the domain name used in private_registry_cert.pem. 
	2. Copy private_registry_cert.pem and private_registry_key.pem to /srv/kubernetes in all nodes. In case, the pod is generated there. 

	sudo mkdir -p /srv/kubernetes
	sudo mv private_registry_cert.pem /srv/kubernetes/
	sudo mv private_registry_key.pem /srv/kubernetes/

	3. Create directory /etc/docker/certs.d/private-registry:31000 and cp cacert.pem to /etc/docker/certs.d/private-registry:31000/ca.crt for all nodes

	sudo mkdir -p /etc/docker/certs.d/private-registry:31000
	sudo mv cacert.pem /etc/docker/certs.d/private-registry:31000/ca.crt

2. Use domain rather than IP (node_ip is the ip of this physical node)
	1. sudo vi /etc/hosts to add "<node_ip> private-registry" for all nodes.  

3. Create glusterfs endpoint and volume
	1. Modify glusterfs endpoint file.
		sudo vi glusterfs-endpoints.json
	2. Create volume with the path indicated in the file private-registry-controller-wth-glusterfs.json
		sudo gluster volume create private_registry_volume replica 3 k8node1:/data/glusterfs/private_registry_volume k8node2:/data/glusterfs/private_registry_volume k8node3:/data/glusterfs/private_registry_volume force
		sudo gluster volume start private_registry_volume
	
4. Deploy
	python create_with_glusterfs.py
	
	
	
	
	
	

	
	
	
	
	
	
# Remove node
1. Gracefully delete the pod on it.
2. Shutdown machine
3. Use kubectl delete node node_name to remove
4. Remove the flannel subnet